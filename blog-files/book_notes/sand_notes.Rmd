---
title: "Statistical Analysis of Network Data - Notes"
author: "Benjamin Draves"
output:
  prettydoc::html_pretty:
    theme: architect
    highlight: github
---

## Chapter 1 - Introduction 

## Chapter 2 - Preliminaries 

## Chapter 3 - Mapping Networks 

## Chapter 4 - Descriptive Analysis of Network Data 

## Chapter 5 - Sampling and Estimation in Network Graphs 

## Chapter 6 - Models for Network Graphs 

#### 6.1 Introduction 
In this section, we focus on constructing a class or model of network graphs. Mathematically, this collection can be formulated by the set $$\{\mathbb{P}_{\theta}(G), G\in\mathcal{G}: \theta\in\Theta\}$$ where $\mathcal{G}$ is a collection of possible graphs and $\mathbb{P}_{\theta}$ is a probability distribution over this class paramterized by the (possibly vector valued) parameter $\theta$. The complexity of these models comes primarily through one of two avenues. (a) the specification of $\mathbb{P}$ or the specification of $\mathcal{G}$. Using these models, we can begin to characterize networks in a rigorous manor. 

#### 6.2 Random Graph Models 

Most common, a \textit{random graph model} is specified by defining a class of networks $\mathbb{G}$ with the uniform distribution over these networks. In traditional sampling theory, there are two major approaches to constructing estimates of parameters. (a) Design-based which focuses on the sampling procedure that generated the subset of observed networks and (b) Model-based which focuses on class of possible networks and infers the relation between the sample and the population. One way in which we can see this is when we try and infer a parameter $\eta(G)$ from which we base our inference off of the sampled version $G^{*}$. If we assume the design based approach, we need not consider a larger class $\mathcal{G}$. In the model based approach we must restrict our inference only to these networks found in $\mathcal{G}$. 
$$\mathcal{G}\overset{unif. sample}{\longrightarrow}G\overset{sampled. version}{\longrightarrow}G^{*}$$ Another clear application is in hypothesis testing. By creating this sampling distribution, we can determine if a characteristic $\eta(G^{*})$ is significant by comparing it to the reference set $\{\eta(G):G\in\mathcal{G}\}$. Enumerating these possibilities is often quite difficult for interesting classes $\mathcal{G}$ so several computational alternatives have been derived.

The \textit{classical random graph models} were established by Erdos and Renyi where they specified a class of random networks with fixed number of edges and vertices $(N_E, N_V)$ with $G\in\mathcal{G}_{N_{V},N_{E}}$ having this property $\mathbb{P}(G) = \binom{N}{N_{E}}^{-1}$ where $N = \binom{N_V}{2}$. In this way, each network with these fixed parameters are assigned equal probabilities. Gilbert designed a similar class of networks where only $N_V$ was fixed and each edge in the network was assigned with probability $p\in(0,1)$. When $p$ is appropriately defined $N_E \sim pN_V$ these two models are asymptotically equivalent. Some properties of Gilbert's model (which is generally refereed to as \textit{the} classical graph model) are given below. 

* If $p = c/N_V$ and $c>1$ then whp, their will be a single connected component with $O(N_V)$ vertices (called the \textit{giant component}) and the other smaller components will only have $O(\log N_V)$. If $c<1$ then whp all components will have the smaller $O(\log N_V)$ size. 
* In the $c>1$ case, the density of the graph will be $p\sim N_V^{-1}$ so $G$ is expected to be sparse for large $N_V$. 
\item Under the sample $p = c/N_V$ model, for large $N_V$ the degree distribution $f_d(\cdot)\sim\text{Pois}(c)$. 
* Under this model, clustering is not expected 
*These networks contain the \textit{small world property}: whp, the diameter varies like $O(\log N_V)$ when $N_V\to\infty$ 


We can generalize the Erdos-Reyni models to attain the \textit{generalized random graph models}. Specifically, we fix $N_V$ and define $\mathcal{G}$ such that each network has some property and then assign equally probability of sampling across this class $\mathcal{G}$. In the classical model, we define one such characteristic being that each edge is assigned with probability $p\in(0,1)$. In the Erdos-Reyni model, we specified $N_E$. Another possible declaration is defining the degree sequence of the network (which then in turn specifies $N_E$ which additional structure). Several different papers/models relating to this degree specification have been developed and studied - mostly with regard to the emergence of giant components and other network typologies.

In order the leverage the models discussed here, sampling/simulation for these networks is crucial. Most such algorithms are designed around the popular MCMC methodology. For the classical random graph model, we could simply generate the $\binom{N_V}{2}$ Bernoulli random variables but if $p\sim N_V^{-1}$ the network is quite sparse and hence a $O(N_V^2)$ is not at all desirable. An $O(N_V + N_E)$ algorithm is developed, where for each vertex, the other $N_V-1$ edges are ordered/indexed. Then a single draw from a geometric RV with parameter $p$ allows for the algorithm to "skip" the possible edges as the waiting time of $N_V-1$ Bernoullis is given by a geometric RV. This process is then repeated for the remaining edges in the network. One possible drawback to this algorithm is simply that edges are now correlated. Another algorithm is given by prespecifying $N_E$ and then simply placing unique edges in the network until this number is attained. This algorithm runs in $O(N_E+N_V)$ time. 

Generating generalized random graphs is usually considerably more difficult. Here we focus on the networks with a specified degree sequence. Two of the most popular are (a) \textit{matching algorithms} and (b) \textit{switching algorithms}. Matching algorithms are explained as follows 

1. Specify an ordered degree sequence $\{d_{(1)},\ldots, d_{(N_V)}\}$ 
2. For each vertex $i = 1,2,\ldots, N_V$, create a list containing a total of $d_{(i)}$ copies of the vertex $i$. 
3. Sample pairs from this list with equal probability
4. Remove the sampled pair for the list 
5. Iterate until the list is empty

In the cases where a vertex is matched with itself, or their are multi-edges, the graph is discarded and the process repeated. This can be quite inefficient in practice so monitoring the pairs selected is usually a practical solution used (even though it produces bias in the sampling sequence). Another alternative is to sample so that no existing pair can be sampled again. Chen, Diaconis, Holmes, and Liu develop an algorithm that treats the problem as a sequential assignment problem and have seen decent results in practice. Whether the algorithm is scalable to sparse graphs is yet to be seen. Switching algorithms are generally more simple and can be explained as follows 

1. Starting from an initial degree sequence $\{d_{(1)}, \ldots, d_{(N_V)}\}$ construct an initial graph $G^{(1)}$
2. Randomly select two edge $e_1 = \{u_1, v_1\}$ and $e_2 = \{u_2, v_2\}$ 
3. Switch the edge to attain $e_1^{*} = \{u_1, v_2\}$ and $e_2^{*} = \{u_2, v_1\}$
4. Remove $e_1, e_2$ and replace with $e_1^{*}$ and $e_2^{*}$ (assuming neither exist) to attain $G^{(2)}$
5. Iterate sufficiently long to attain sample 

This algorithm falls with the MCMC category so the limiting theory guarantees convergence (but no such rate or assumption testing really exists). Other MCMC type algorithms exist for other specifications of the generalized random graphs. 

Some of the primary reasons that we develop these probabilistic models is to use them statistical in estimation and significance assessment. Here they present a MOM type development of model parameters in the classical random models setting where the goal of inference is to uncover $N_V$. By estimating first $p$ then an estimate of $N_V$ is developed. Additionally, if the goal is to estimate some characteristic $\eta(G)$, we could instead just building estimators for the class $\{\eta(G):G\in\mathcal{G}\}$ as opposed to using a pluggin estimator $\eta(\widehat{G})$. For assessing significance, we could use our model to build a reference distribution but as we noted above, there are sever computational concerns with this approach. Some of these issues can be remedied by placing more restrictive conditions on the generalized random graph models to reduced the computational time to process this reference distribution (e.g. number of vertices in each group). 

For a concrete example, the consider the karate network with the goal to test to the significance of the network clustering coefficient. In this, they build two reference distributions - one with a fixed number of edges and one with a fixed degree distribution. From here, they bootstrap the $\eta(\cdot)$ statistic to preform statistical "testing" and significance assessment. Another application is in \textit{motif identification} where a \textit{motif} is defined to be a subgraph structure that reappears throughout a collection of observed networks. (In some sense motifs, define the building blocks of in which networks are constructed.) They do so by calculating relative frequencies of observed subgraphs $L_k$ over $k$ vertices. Similar to the karate network, they define a reference distribution in order to find those extreme distributions of interest. Clearly, there are sever computational concerns with this approach to network analysis, but there are certain sampling techniques that offer some good solutions to these problems. 

#### 6.3 Small-World Models

#### 6.5 Exponential Random Graph Models (ERGMs)

Most of the models considered up to this point fail to encapsulate some of the primary objectives that a statistical model should have. An ERGM attempts to adapt the celebrated exponential family to the random network framework. From here, we may use results derived in more classical settings for this datastructure. 

A random vector $\mathbf{Z}$ is said to belong to an _exponential family_ if its pmf can be expressed as follows $$\mathbb{P}_{\theta}(\mathbf{Z} = \mathbf{z}) = \exp\{\theta^{T}\mathbf{g}(\mathbf{z}) - \psi(\theta)\}$$ where $\theta\in\mathbb{R}^{p\times 1}$ is a vector of parameters, $\mathbf{g}(\cdot)$ is a $p$-dimensional function, and $\psi(\theta)$ is a normalization term. Now extending this framework to network data, suppose that we have a graph $G = (V,E)$ with $Y_{ij} = Y_{ji}$ be a binary random variable indicating the presence of an edge. Thus, $\mathbf{Y} = (Y_{ij})$ is the random adjacency matrix of $G$. Let $\mathbf{y} = (y_{ij})$ be a random realization of $\mathbf{Y}$. An ERGM is then a specification of the joint distribution of the elements of $\mathbf{Y}$ given by $$\mathbb{P}_{\theta}(\mathbf{Y} = \mathbf{y}) = \left(\frac{1}{\kappa}\right)\exp\left\{\sum_{H}\theta_{H}g_H(\mathbf{y})\right\}$$ 
where 

* $H$ is a _configuration_ : set of possible edges among a subset of vertices in $G$
* $g_H(\mathbf{y}) = \prod_{y_{ij}\in H}y_{ij} \in \{0,1\}$ which is zero of the configuration is in $\mathbf{y}$ and zero otherwise
* $\theta_H\neq 0$ means that, conditional on the rest of the graph, $\mathbf{Y}_{ij}$ are dependent for all pairs of vertices in $H$
* $\kappa$ a normalizing constant 

This model has a pretty complex dependence structure due to the third bullet point here. It assumes that subset of edges are independent of one another given the remaining edges. See the **Hammersley - Clifford Theorem** for more conditions on this probability structure. 

**Ex**: Suppose that $Y_{ij}\perp Y_{i'j'}$. The means that $\theta_H = 0$ for $|H|>2$ (see the third bullet above). Therefore, the only function of interest $g_H(\mathbf{y}) = y_{ij}$ and our ERGM is given by $$\mathbb{P}_{\theta}(\mathbf{Y} = \mathbf{y}) = \left(\frac{1}{\kappa}\right)\exp\left\{\sum_{(i,j)}\theta_{ij}y_{ij}\right\}$$ Notice that this model specifics every single parameter $p_{ij} = \text{logit}(\theta_{ij})$ resulting in $N_V^2$ parameters. To reduce the number of parameters, we assume homogeneity across $G$ such that $\theta_{ij}  \equiv \theta$ which results in the model $$\mathbb{P}_{\theta}(\mathbf{Y} = \mathbf{y}) = \left(\frac{1}{\kappa}\right) \exp\{\theta L(\mathbf{y})\}$$ where $L(\mathbf{y}) = \sum_{ij}y_{ij} = N_e$ which is the classic Bernoulli RGM. 

**Ex**: If we make the same assumptions in the previous model, but assume that the vertices comprise two different classes $S_i$, $i=1,2$. Here we could impose homogeneity withing and between sets and arrive at the model of the form 

$$\mathbb{P}_{\theta}(\mathbf{Y} = \mathbf{y}) = \left(\frac{1}{\kappa}\right) \exp\{\theta_{11} L_{11}(\mathbf{y}) + \theta_{22} L_{22}(\mathbf{y}) + \theta_{12} L_{12}(\mathbf{y})\}$$ where $L_{ij}(\mathbf{y})$ is the number edges between group $i$ and $j$ and where the probability model is homogeneous across these sets. 

**Ex**: Extending dependency structure, the _Markov Dependence_ for network graph models specifies that two edges are dependent if they share a common vertex. A random graph with property is called a _Markov Graph_. Frank and Strauss show that a graph is a Markov graph if and only if its probability function can be expresses follows 
$$\mathbb{P}_{\theta}(\mathbf{Y} = \mathbf{y}) = \left(\frac{1}{\kappa}\right)\exp\left\{\sum_{k=1}^{N_v -1}\theta_kS_k(\mathbf{y}) + \theta_{\tau}T(\mathbf{y})\right\}$$ where 

* $S_1(\mathbf{y}) = N_e$ is the number of edges 
* $S_k(\mathbf{y})$ is the number of _k-starts_: a tree with one vertex of degree $k$ and $k$ vertices of degree 1
* $T(\mathbf{y})$ is the number of triangles

$\theta_k$ is successively higher-order effects in the model for larger values of $k$. Clearly fitting this model for larger values of $k$ is oftentimes computationally unfeasible and difficult to interpret. One solution is to put a parameter constraint on the $\theta_k$ such that $\theta_k\propto (-1)^k\lambda^{2-\lambda}$ for all $k\geq 2$ and for $\lambda\geq 1$. This approach combines all $k\geq 2$ stars terms into a single term given by $$AKS_{\lambda}(\mathbf{y}) = \sum_{k = 2}^{N_v - 1}(-1)^{k}\frac{S_k(\mathbf{y})}{\lambda^{k-2}}$$ with corresponding parameter $\theta_{AKS}$. This, and other parametric constraints, offer different relations to the weighted degree counts and weight this distribution in different ways. 

The ERGM framework also allows for incorporation of vertex specific information. Instead, you can specific the conditional distribution $\mathbb{P}_{\theta}(\mathbf{Y} = \mathbf{y}|\mathbf{X} = \mathbf{x})$ typical in most regression settings. 

To estimate the parameters $\theta_H$ most statisticians use the MLE framework. But with the highly dependent structure, asymptotic results have yet to be developed. The goal of most MLE's is to maximize $$\ell(\theta) = \theta^T\mathbf{g}(\mathbf{y}) - \psi(\theta)$$ where $\psi(\theta) = \log(\kappa(\theta))$. Taking derivatives one each side and using $\psi(\theta)$ as a cumulant function, we see that the MLE is the solution to the set of equations $$\mathbb{E}_{\hat{\theta}}(\mathbf{g}(\mathbf{Y})) = \mathbf{g}(\mathbf{y})$$ But notice that since $\psi(\theta)$ is a sum over $2^{\binom{N_v}{2}}$, working with this explicitly is unfeasible especially when we must do this for multiple values of $\theta$. Two MCMC solutions exist: one that approximates $\ell(\theta)$ and one that approximates the solutions to $\mathbb{E}_{\hat{\theta}}(\mathbf{g}(\mathbf{Y})) = \mathbf{g}(\mathbf{y})$. Each algorithm relies on being able to sample from the model itself which is typically done via a Gibbs sampler. Other computational approaches to this problem exist such as maximizing the psuedo-likelihood. 

Model fitting currently in this class of models is done mostly by comparing the fit model to several random draws from the distribution. The authors also comment on model degeneracy - the result of a model fit putting much mass on the empty or complete graph. There are several sources that look at this problem in some depth. 

## Chapter 7 - Network Topology Inference 

## Chapter 8 - Modeling & Prediction for Processes on Network Graphs 

## Chapter 9 - Anaysis of Network Flow Data

## Chapter 10 - Graphical Models 




















