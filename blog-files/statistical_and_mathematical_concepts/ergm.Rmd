---
title: "Exponential Random Graph Models"
author: "Benjamin Draves"
output:
  prettydoc::html_pretty:
    theme: architect
    highlight: vignette
---

##Motivation 

In almost every statistical setting, a formal probability is introduced and developed in an attempt to standardize inference. In (generalized) linear modeling, statisticians make an explicit assumption about the form the conditional distribution $\mathbf{Y}|\mathbf{X}$ which allows for a unified approach to model fitting, inference, and diagnostics. In some instances, such as quasi-likelihood approaches, these models are simplified to include only the specifications required in the fitting portion of the model. In any case, developing and analyzing a class of probability models in connection to a statistical application allows for a consistent approach to inference and modeling fitting. 

In classical mathematical statistics, several results require that the sample in question comes independently and identically distributed from an _exponential family_. This class of distributions offers several desirable algebraic properties useful for identifying sufficient statistics and formulating hypothesis tests. In an effort to extend this class of models to network type data, the _Exponential Random Graph Model (ERGM)_ assumes a similar form of probability distribution of a graph's adjacency matrix. This model assumption, along with appropriate specifications, allows for a uniform approach to fitting and inference connecting a network's observed topological structure with the underlying probability model. 

***
## The Model 

Let $G = (V,E)$ be a random network with $Y_{ij} = Y_{ji}$ be a Bernoulli random variable representing the presence of edge $e_{ij}\in E$. With this, we define the random adjacency matrix $\mathbf{Y} = (Y_{ij})_{1\leq i,j\leq n}$. Let $\mathbf{y}$ be a particular realization of $\mathbf{Y}$. 

> The _Exponential Random Graph Model_ is the specification of the joint distribution of $\mathbf{Y}$ given by $$\mathbb{P}_{\theta}(\mathbf{Y} = \mathbf{y}) =\kappa(\theta)^{-1} \exp\left\{\sum_{H}\theta_{H}g_H(\mathbf{y})\right\}$$ 
>where 
>
> * $H$ is a _configuration_ : an element of the powerset of $E$
> * $g_H(\mathbf{y}) = \prod_{y_{ij}\in H}y_{ij}$ is one if $H$ occurs in $\mathbf{y}$ or zero otherwise
> * If, conditional on the rest of the graph, each $Y_{ij}\in H$ are pairwise dependent then $\theta_H \neq 0$.
> * $\kappa(\theta) = \sum_{\mathbf{y}}\exp\left\{\sum_{H}\theta_{H}g_H(\mathbf{y})\right\}$ is a normalizing constant. 

In this model, the configuration, $H$, plays a major role in determining probability structure of this model. In fact, through the definition of $\theta_H$ and $g_H(\cdot)$, probability can only be accumulated for a particular model if (a) the configuration exists in the network and (b) the edges in the configuration are pairwise dependent random variables. In this way, this model attributes more probability to networks that contain dependent substructures _of any scale_. Lastly, the values of $\theta_H$, the parameters in this model, can be interpreted as the the effect of the presence of this intra-dependent substructure, $H$.  

***
## Examples

> (**Bernoulli Random Graphs**) Assume that $Y_{ij}\perp Y_{jk}$ for $i \neq k$. Then we see that $g_H(\cdot) = 0$ for all configurations $H$ with more that two vertices. Moreover, for configurations $H$ with two vertices, $g_H(\mathbf{y}) = y_{ij}$. That is $g_H(\cdot)$ simplify identifies the presence an edge in $\mathbf{y}$. With these observations, the probability model reduces to the following $$\mathbb{P}_{\theta}(\mathbf{Y} = \mathbf{y}) = \kappa(\theta)^{-1}\exp\left\{\sum_{i,j}\theta_{ij}y_{ij}\right\}$$
Notice, that with this particular model specification, we introduce $N_V^2$ number of parameters. To reduce the number of parameters, we typically make assumptions regarding the structure of the model. For example, by assuming homogenity across the entire network, the model reduces further to $$\mathbb{P}_{\theta}(\mathbf{Y} = \mathbf{y}) = \kappa^{-1}(\theta)\exp\left\{\theta\sum_{i,j}y_{ij}\right\}$$ which is clearly closely related to a random sample of iid Bernoulli Random Variables. 
>
> Notice that we need not assume homogenity across the entire network. For instance, if the network follows a stochastic block modle structure, we can instead assume homogentity across these groups. Suppose that each vertex belongs to one of $K$ groups. Then by assuming homogenity of the within and between group structure the model becomes $$\mathbb{P}_{\theta}(\mathbf{Y} = \mathbf{y}) = \kappa^{-1}(\theta)\exp\left\{
\sum_{k = 1}^{K}\theta_{kk}L_{kk}(\mathbf{y}) + \sum_{k\neq \ell}\theta_{k\ell}L_{k\ell}(\mathbf{y})
\right\}$$ where $\theta_{k\ell}$ is the within/between group effect and $L_{k\ell}$ is the number of edges within and between groups. 

> (**Markov Random Graphs**) Assume that $Y_{ij}\perp Y_{k\ell}$ for $i,j\neq k,\ell$. That is edges are dependent if they share a vertex. This specification is called _Markov Dependence_ and a network that has this probablistic structure is entitled a _Markov Graph_. With help of the _Hammersly-Clifford Theorem_ and assuming homogentity, the ERGM reduces to the following $$\mathbb{P}_{\theta}(\mathbf{Y} = \mathbf{y}) = \kappa(\theta)^{-1}\exp\left\{\sum_{k = 1}^{N_v -1}\theta_kS_k(\mathbf{y}) + \theta_{\tau}T(\mathbf{y})\right\}$$
where $S_1(\mathbf{y})$ is the number of edges, $S_k(\mathbf{y})$ is the number of $k$-stars, and $T(\mathbf{y})$ is the number of triangles. This model clearly illustrates how the ERGM models topologocial features of all scale; from the number of triangels, number of edges, and number of $k$-stars each having their own parameter, these dependency structures are all encapsulated in this model. We view $\theta_k$ as progressively higher order terms for increasing $k$.

***
## Parametric Constraints

In several applications, fitting a full _Markov Graph_ model is quite difficult past the $k=2$ case. Both identifying these structures in the network as well as fitting the model parameters are quite computationally taxing. As an alternative, statisticians will frequently assume a parameter form that combines that $k$-star statistics for $k\geq 2$ into a single statistic with a single associate parameter. Some popular choices for this parametric form are given below.

> The _alternating k-star statistic_ attempts to take into account the star effects of all scale insto a single statistic given by $$AKS_{\lambda}(\mathbf{y}) = \sum_{k=2}^{N_v - 1}(-1)^k\frac{S_k(\mathbf{y})}{\lambda^{k-2}}$$
for $\lambda \geq 1$. This statistic is closely connected with the network's degree count. Indeed, it is a linear function of the _geometrically weighted_ degree count. With this insight, this statistic attempts to model the degree distribution where $\lambda$ represents the likelihood of presence of higher-degree vertices. 

> The _alternating k-triangle statistic_ attempts to model the affect of $k$-traingles with the statistic $$AKT_{\lambda}(\mathbf{y}) = 3T_1 + \sum_{k = 2}^{N_v -2}(-1)^{k+1}\frac{T_k(\mathbf{y})}{\lambda^{k-1}}$$ where $T_k$ is the number of $k$-triangles which sets of $k$ traingles that share the same base. 

With these parametric constraints, statisticians will often model Markov Random Graphs through the following model. 
$$\mathbb{P}_{\theta}(\mathbf{Y} = \mathbf{y}) = \kappa(\theta)^{-1}\exp\left\{\theta_1S_1(\mathbf{y})+ \theta_{AKS(T)}AKS(T)_{\lambda}(\mathbf{y})\right\}$$
where $\theta_{1}$ represents the effect of the density of the network and $\theta_{AKS(T)}$ represents the effect of the transitivity of the network. While the statistics presented here have several desirable properties, other parametric forms that highlight or attempt to capture a different topological structure can be used and interpreted in a similar fashion. 

***
## Vertex Specific Attributes

The ERGM developed here only studies the structure of the observe red network. In several applications, practitioners frequently have vertex level covariates that should be incorporated into the model fitting procedure. By extending the ERGM, we can instead specify a conditional distribution of the network that relies on these covariates values $\mathbf{X}$. For simplicity, we only extend the Markov Random Graph here. 
$$\mathbb{P}_{\theta,\beta}(\mathbf{Y} = \mathbf{y}|\mathbf{X} = \mathbf{x}) = \kappa(\theta)^{-1}\exp\left\{\sum_{k=1}^{N_v - 1}\theta_kS_k(\mathbf{y}) + \theta_{\tau}T(\mathbf{y}) + \beta^T\mathbf{g}(\mathbf{y},\mathbf{x})\right\}$$
where $\mathbf{g}(\mathbf{y},\mathbf{x}) = \sum_{1\leq i <j \leq N_v}\gamma_{ij}h(\mathbf{x}_i, \mathbf{x}_j)$ where $h(\cdot, \cdot)$ is a symmetric function that combines the attributes of vertex $i$ and vertex $j$ when modeling $Y_{ij}$. By specifying the model in this way, ERGM's extend into the regression model setting and can be quite useful when considering network regression problems.

***
## Conclusions & Future Topics

ERGMs extend the familiar structure of an exponential random family to network data. Through its construction, the probability structure is specified primarily by identifying dependent random variables as well as topological features of the network. Moreover, but making homogeneity assumptions and imposing parametric constraints on the model, there are few number of model parameters with clear interpretations. Lastly, by including vertex specific information, we can extend this class of models to the network regression realm. 


This post was primarily focused on introducing ERMGs and not on model fitting procedures. In the future, an overview of the MCMC algorithms that provide parameter estimates would be an interesting topic to overview/implement. Lastly, several of the model reductions are only possible by use of the _Hammersley-Clifford Theorem_. An overview of this theorem's proof may prove useful. 





