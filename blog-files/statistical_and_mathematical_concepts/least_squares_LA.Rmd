---
title: "Least Squares - A Linear Algebraic Perspective"
author: "Benjamin Draves"
output:
  prettydoc::html_pretty:
    theme: architect
    highlight: vignette
---

##Motivation 

In several statistical and mathematical applications, we wish to approximate a lower dimensional structure that describes the a particular object(s) in question. In mathematics, we may encounter an inconsistent system of linear equations described by the equation $\mathbf{Ax} = \mathbf{b}$ where $\mathbf{A}\in\mathbb{R}^{m\times n}$, $\mathbf{x}\in\mathbb{R}^{n\times 1}$, and $\mathbf{b}\in\mathbb{R}^{m\times 1}$ yet wish to approximate a the best solution with respect to the $\ell_2$ penalty by solving the following minimization problem $$\hat{\mathbf{x}} = \underset{x\in\mathbb{R}^{n\times 1}}{\min}||\mathbf{Ax} - \mathbf{b}||_2^2$$ Similarly, in statistics, this problem is most frequently encountered in ordinary linear regression where we look to approximate $\mathbb{E}(\mathbf{Y}|\mathbf{X})$ by a linear function of the covariates, $\mathbf{X}\beta$. Here, the parameters of the model are given by the $\beta$ coefficients as well as the conditional variance $\text{Var}(\mathbf{Y}|\mathbf{X}) = \Sigma$. In a similar fashion, we look to find the value of parameters that minimize the $\ell_2$ penalty $$\hat{\beta} = \underset{\beta\in\mathbf{R}^{p\times 1}}{\min}||\mathbf{Y} - \mathbf{X}\beta||_2^2$$ Equivalently, OLS looks to maximize the associated likelihood if we assume that the errors follow a normal distribution $\mathbf{Y} - \mathbf{X}\beta \sim N(\mathbf{0}, \sigma^2\mathbf{I})$. 

In several courses where optimization or regression is discussed, these minimization problems are solved directly with the use of vector calculus. While this approach solves the problem quickly, perhaps a more natural approach to understanding the corresponding solution is to consider the underlying linear algebraic structure. In this way, we can see why which solutions preform better than others, why multiple solutions can exist, and see how we can generalize these ideas to different $\ell_p$ spaces.  

***
## Preliminaries

Before we develop the solution to the least squares problem, we review some basic concepts of orthogonal projections as well as bases and their construction. 

> **Def:** Let $\mathbf{u,v}\in\mathbf{V}$, an inner product space. The _orthogonal projection_ of $\mathbf{u}$ onto $\mathbf{v}$, denoted $\text{proj}_{\mathbf{v}}\mathbf{u}$ is given by  $$\text{proj}_{\mathbf{v}}\mathbf{u} = \frac{\langle\mathbf{u},\mathbf{v}\rangle}{\langle\mathbf{v},\mathbf{v}\rangle}\mathbf{v}$$

> **Def:** Let $\mathbf{B} = \{v_1, v_2, \ldots, v_n\}$ be a basic for in the inner product space $\mathbf{V}$. Then $\mathbf{B}$ is called an _orthogonal basis_ if $\langle \mathbf{v}_i, \mathbf{v}_j\rangle = 0$ for all $i\neq j$. In addition if $\langle \mathbf{v}_i, \mathbf{v}_i\rangle = 1$ for all $1\leq i\leq n$ then $\mathbf{B}$ is called an _orthonormal basis_. 

While not all bases are orthogonal or orthonormal, it is always possible to construct an orthonormal basis $\mathbf{B}'$ from $\mathbf{B}$. This is the focus of the _Gram-Schmidt Process_. The _Gram-Schmidt Process_ constructs an orthonormal basis by alliteratively 'removing' the colinearity between vectors in the basis set. The process can be described as follows. 

0. Let $\mathbf{B} = \{\mathbf{v}_1, \ldots, \mathbf{v}_n\}$ be a basis for an inner product space $\mathbf{V}$
1. Define $\mathbf{w}_i = \mathbf{v}_i - \sum_{j = 1}^{i-1}\frac{\langle \mathbf{v}_i, \mathbf{w}_i\rangle}{\langle \mathbf{w}_j, \mathbf{w}_j\rangle}\mathbf{w}_j$ for $1\leq i \leq \text{dim}(\mathbf{V})$
2. Define $\mathbf{B}' =\left\{\frac{\mathbf{w}_1}{||\mathbf{w}_1||}, \ldots , \frac{\mathbf{w}_n}{||\mathbf{w}_n||}\right\}$

>**Theorem:** The construted basis $\mathbf{B}'$ from the Grahm-Schmidt Process is an orthonormal basis for the inner product space $\mathbf{V}$. 

***
## Orthogonal & Fundamental Subspaces 

In the least squares problem we look to find a vector $\mathbf{x}$ such that $\mathbf{Ax}$ is close to $\mathbf{b}$. If $\mathbf{b}\in\text{col}(A)$ then by definition there exists $\mathbf{x}$ such that $\mathbf{Ax} = \mathbf{b}$. However, when $\mathbf{b}\not\in\text{col}(\mathbf{A})$, we seek an approximation in $\text{col}(\mathbf{A})$ that is as close to $\mathbf{b}$ as possible. If $\text{dim}(\text{col}(\mathbf{A})) = 1$, then the closest vector is the projection of $\mathbf{u}$ onto $\mathbf{v}$, the vector characterizing the column space of $\mathbf{A}$. However, we when $\text{dim}(\text{col}(\mathbf{A})) > 1$, we need to generalize this idea of an orthogonal projection to an entire _space_, not just a single vector. 

> **Def:** Let $\mathbf{S}\subseteq \mathbb{R}^{n}$ be a subspace with corresponding orthonormal basis $\{\mathbf{v}_1, \ldots, \mathbf{v}_t\}$. Then the projetion of a vector $\mathbf{u}\in\mathbb{R}^n$ onto $\mathbf{S}$, called a _subspace projection_, denoted $\text{proj}_{\mathbf{S}}\mathbf{u}$ is given by $$\text{proj}_{\mathbf{S}}\mathbf{u} = \sum_{j = 1}^{t}\langle\mathbf{u},\mathbf{v}_j\rangle\mathbf{v_j}$$

While we now have a sense of how to find our solution, this projection is in no way constructive. By studying the part of $\mathbf{b}$ that lies outside of $\text{col}(\mathbf{A})$, we can find the form of $\hat{\mathbf{x}}$. We begin with studying the orthogonal complements. 

> **Def:** The _orthogonal complement_ of a subspace $\mathbf{S}\subseteq\mathbb{R}^n$ is given by $$\mathbf{S}^{\perp} = \{\mathbf{v}\in\mathbf{R}^{n}: \mathbf{v}\cdot\mathbf{u} = 0, \forall \mathbf{u}\in\mathbf{S}\}$$

One of the several nice properties of this object is that for any inner product space $\mathbf{V} = \mathbf{S}\oplus\mathbf{S}^{\perp}$. In our particular case, we see that $\mathbf{b}$ can written the sum of a vectors in $\text{col}(\mathbf{A})$ and $\text{col}(\mathbf{A})^{\perp}$. Moreover, by studying the relationship between $\text{col}(\mathbf{A})$ and $\text{col}(\mathbf{A})^{\perp}$ we can begin to find a constructive solution to this problem. This relationship is described through the Fundamental Subspaces Theorem which we now discuss. 

> **Def:** The four _fundamental subspaces_ of a matrix $\mathbf{A}\in\mathbb{R}^{m\times n}$ are $N(\mathbf{A})$, $N(\mathbf{A}^{T})$, $\text{col}(\mathbf{A})$, and $\text{col}(\mathbf{A}^{T})$. 

> **Theorem:** If $\mathbf{A}\in\mathbb{R}^{m\times n}$ then the following hold: $\text{col}(\mathbf{A})$ and $N(\mathbf{A}^T)$ are orthogonal subspaces of $\mathbb{R}^m$ with $\text{col}(\mathbf{A}) \oplus N(\mathbf{A}^T) = \mathbb{R}^m$. Similarly, $\text{col}(\mathbf{A}^T)$ and $N(\mathbf{A})$ are orthogonal subspaces of $\mathbb{R}^n$ with $\text{col}(\mathbf{A}^T) \oplus N(\mathbf{A}) = \mathbb{R}^n$

> **Proof:** Let $\mathbf{v}\in\text{col}(\mathbf{A})$ and $\mathbf{u}\in N(\mathbf{A}^T)$. Now, notice by definition $\mathbf{v} = \mathbf{Ax}$ for some $\mathbf{x}\in\mathbb{R}^{n\times 1}$ and by definition $\mathbf{A}^T\mathbf{u} = 0$. Now, considering $\mathbf{v}\cdot\mathbf{u} = \mathbf{v}^T\mathbf{u}$ we have the following
$$\mathbf{v}^T\mathbf{u} = (\mathbf{Ax})^T\mathbf{u} = \mathbf{x}^T(\mathbf{A}^T\mathbf{u}) = 0$$ Hence we see that $\text{col}(\mathbf{A})\perp N(\mathbf{A}^T)$. To see why these spaces decompose $\mathbb{R}^{m}$ notice that the $$\mathbb{R}^{m} = \text{col}(\mathbf{A}) \oplus \text{col}(\mathbf{A})^{\perp}$$ Moreover as $\text{col}(\mathbf{A})^{\perp} = N(\mathbf{A}^{T})$ we see that 
$$\mathbb{R}^{m} = \text{col}(\mathbf{A}) \oplus N(\mathbf{A}^T)$$ By simply changing symbols, one can aruge that $$\mathbb{R}^{n  } = \text{col}(\mathbf{A}^T) \oplus N(\mathbf{A})$$


***
## The Least Squares Solution

Now, having the appropriate tools to answer the least squares problem, recall that we are trying to solve the following minimization problem $$\underset{x\in\mathbb{R}^{n}}{\min}||\mathbf{Ax} - \mathbf{b}||_2^2$$ Let $\mathbf{S} = \text{col}(\mathbf{A})$ and $\mathbf{A}\hat{\mathbf{x}} = \text{proj}_{\mathbf{S}}\mathbf{b}$. Notice that by properties of orthogonal projections that $\mathbf{A}\hat{\mathbf{x}}$ minimizes the $\ell_2$ distance above. We now turn to finding an expression for $\hat{\mathbf{x}}$. 

By definition, $\mathbf{A}\hat{\mathbf{x}} - \mathbf{b} = \text{proj}_{\mathbf{S}}\mathbf{b} - \mathbf{b} \not\in \mathbf{S}^{\perp}$. Now notice that $\mathbf{S}^{\perp} = N(\mathbf{A}^{T})$. With this we can write the following, 
$$\mathbf{A}^{T}(\mathbf{A}\hat{\mathbf{x}} - \mathbf{b}) = 0$$
Which upon rearranging yields the _normal equations_ $$\mathbf{A}^T\mathbf{A}\hat{\mathbf{x}} = \mathbf{A}^{T}\mathbf{b}$$ Assuming that $\mathbf{A}^T\mathbf{A}$ is invertible, these equations yield a unique solution given by $$\hat{\mathbf{x}} = (\mathbf{A}^{T}\mathbf{A})^{-1}\mathbf{A}^T\mathbf{b}$$

***
##Conclusions & Future Topics

In several developments of the least squares solutions, we tend to focus on the minimization problem at hand. While stochastic and mathematical optimization has is appropriate here, by approaching this problem through a linear algebraic framework, we can see how the underlying subspaces affect the solutions. Here we use the Fundamental Subspace theorem to argue that this problem can be solved directly by orthogonal projection mappings. Without the use of any calculus related optimization techniques, we can still arrive at the optimal solution in a much more illustrative argument. 

In future work, considering a general $\ell_p$ penalization function may be an interesting extension. The orthogonal projections argument would alter to general $\ell_p$ spaces should be solvable for $p>1$. Moreover, considering penalized regression problems in this context would be an interesting extension. 






